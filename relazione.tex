\documentclass[a4paper,12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{braket}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{hyperref}

\title{Crittografia Post-Quantum}
\author{Olivieri Michele}
\date{\today}

\begin{document}

\maketitle

\newpage

\tableofcontents

\newpage

\section{Introduzione}

\subsection{Contesto e motivazioni} 

Per comprendere l'importanza della crittografia post-quantistica, è fondamentale analizzare 
il contesto in cui essa si inserisce e le motivazioni che ne hanno guidato lo sviluppo.

La crittografia classica, come visto a lezione, pone le sue fondamenta su problemi computazionalmente difficili 
per i quali non esistono algoritmi efficienti in grado di risolverli in tempi polinomiali.

Tuttavia, l'emergere dei computer quantistici ha introdotto una nuova dimensione nel panorama della sicurezza informatica.
Questi dispositivi sfruttando i principi della meccanica quantistica sono in grado di eseguire calcoli in modo radicalmente diverso 
rispetto ai computer classici, rendendoli potenzialmente capaci di risolvere in tempi polinomiali quei problemi matematici 
che risultano intrattabili per le macchine convenzionali.

\subsection[Minaccia dei computer quantistici]{Minaccia dei computer quantistici: Perché è necessaria?}

L’ipotesi di una minaccia quantistica emerge già nel ventesimo secolo, quando nel 1994 l’algoritmo di Shor dimostra che un 
computer quantistico sufficientemente potente potrebbe fattorizzare grandi numeri interi e calcolare logaritmi discreti 
in tempo polinomiale. Questo rende vulnerabili algoritmi come RSA, ECC e DSA, che costituiscono la base della sicurezza 
informatica moderna.

Per molti anni il problema è rimasto solo teorico, perché il calcolo quantistico non aveva applicazioni pratiche. 
Lo scenario è cambiato con i recenti progressi nel settore e con lo sviluppo dei primi prototipi di calcolatori quantistici 
da parte di aziende come Google e Microsoft, che hanno riportato risultati significativi con i loro progetti: Myorana 1 e Willow. 
Questi progetti hanno quindi portato alla luce dei veri calcolatori quantistici funzionanti e nonostante il limitato numero di qbit 
stabili sia insufficiente per applicazioni pratiche su larga scala, questo segna una svolta nel settore.

Sebbene le macchine quantistiche siano ancora in fase sperimentale, diversi scienziati ritengono che la loro costruzione 
su larga scala sia ormai una sfida principalmente ingegneristica, e alcuni prevedono la loro maturazione entro i prossimi vent’anni. 

Considerando che l’attuale infrastruttura crittografica ha richiesto quasi due decenni per essere implementata, risulta necessario 
iniziare da subito la transizione verso sistemi progettati per resistere al calcolo quantistico.

\subsection{Obiettivi della crittografia post-quantistica}

L'obiettivo della crittografia post-quantistica è quindi sviluppare algoritmi crittografici 
sicuri sia contro i computer quantistici che classici, garantendo così un futuro sicuro 
anche in un'era dominata dai computer quantistici.

La crittografia post-quantistica non si limita a sostituire gli algoritmi vulnerabili, 
ma mira a costruire un'infrastruttura di sicurezza robusta e duratura, 
capace di adattarsi alle sfide tecnologiche future mantenendo la compatibilità con 
i protocolli e le reti esistenti.

\newpage

\section{Fondamenti della crittografia classica}

Per comprendere le sfide poste dai computer quantistici alla crittografia moderna, è fondamentale 
conoscerne i principi di base dietro i principali algoritmi crittografici attualmente in uso.
Una volta capiti tali principi, procederemo ad analizzare Shor, il suo impatto sugli algoritmi ed
infine le soluzioni proposte dalla crittografia post-quantistica.

\subsection{Crittografia a chiave pubblica: RSA, ECC}

La crittografia a chiave pubblica, introdotta nel 1976 da Diffie, Hellman e Merkle, costituisce 
una svolta fondamentale nel panorama della sicurezza informatica moderna. A differenza dei sistemi 
simmetrici, che vincolano mittente e destinatario alla condivisione di un unico segreto, i protocolli 
asimmetrici impiegano una coppia di chiavi: una pubblica $k_{\text{pub}}$, liberamente distribuibile, 
e una privata $k_{\text{prv}}$, mantenuta segreta dal proprietario. 
La sicurezza di questo sistema si basa sulla difficoltà di risalire alla chiave privata a partire 
da quella pubblica. Le funzioni di cifratura $C$ e decifratura $D$ sono note a tutti, e 
per ogni messaggio $m$ deve valere:
\[
D(C(m;k_{\text{pub}});k_{\text{prv}})=m.
\]

Il funzionamento di questo sistema si basa sulle funzioni one-way trapdoor: operazioni matematiche 
semplici da eseguire in una direzione, ma computazionalmente intrattabili da invertire senza 
la conoscenza di un'informazione specifica (la "trappola").

La teoria dei numeri e l'algebra modulare forniscono il substrato matematico necessario per 
generare tali funzioni; a seconda del problema matematico sottostante, si distinguono i 
vari algoritmi di crittografia asimmetrica oggi in uso.

\paragraph{Richiami di algebra modulare}

L'aritmetica modulare è un sistema in cui i numeri si riavvolgono entro un intervallo fissato da un modulo $n$. 
Quando un valore supera (o scende sotto) questo intervallo, viene riportato all’interno prendendo 
il resto della divisione per $n$.

Un esempio quotidiano è l’orologio: in un sistema a 12 ore il modulo è $12$. Se sono le
$10$ e aggiungo $4$ ore, il risultato non è $14$, ma $2$, perché:
\[
14 \equiv 2 \pmod{12}.
\]

Per calcolare $c = a \bmod b$ si considera il resto della divisione intera tra $a$ e $b$,
ottenendo un valore sempre compreso tra $0$ e $b-1$, esempio: $6 \bmod 4 = 2$.

\paragraph{Problemi difficili}
\begin{itemize}
    \item \textbf{Fattorizzazione:} dati $p,q$ è facile calcolare $n=pq$; dato $n$ è difficile trovare $p$ e $q$.
    \item \textbf{Radice modulare:} dato $y=x^z \bmod s$ invertire la potenza è difficile senza conoscere $\varphi(s)$.
    \item \textbf{Logaritmo discreto:} data $y=x^z \bmod s$ trovare $z$ è computazionalmente difficile.
\end{itemize}

\subsubsection{RSA}

Il cifrario RSA, proposto da Rivest, Shamir e Adleman nel 1978, è il sistema crittografico a 
chiave pubblica più diffuso e studiato. La sua sicurezza si fonda sulla difficoltà computazionale 
della fattorizzazione di numeri interi molto grandi.

\paragraph{Generazione delle chiavi} 

Ogni utente genera la propria coppia di chiavi attraverso i seguenti passaggi:

\begin{enumerate}
    \item Scelta di due numeri primi $p$ e $q$ molto grandi
    \item Calcolo di $n = pq$ e della funzione di Eulero $\phi(n) = (p-1)(q-1)$
    \item Scelta di un intero $e$ minore di $\phi(n)$ e coprimo con esso
    \item Calcolo dell'intero $d$, inverso moltiplicativo di $e$ modulo $\phi(n)$
\end{enumerate}

La chiave pubblica è la coppia $(e, n)$, mentre la chiave privata è $d$. 
La cifratura di un messaggio $m$ avviene calcolando $c = m^e \bmod n$, 
mentre la decifratura richiede il calcolo di $m = c^d \bmod n$.

La correttezza dell'algoritmo è garantita dal teorema di Eulero: 
poiché $ed \equiv 1 \pmod{\phi(n)}$, si ha $ed = 1 + k\phi(n)$ per qualche intero $k$, e quindi:
$$m^{ed} \bmod n = m^{1+k\phi(n)} \bmod n = m \cdot (m^{\phi(n)})^k \bmod n = m \bmod n$$

\paragraph{Sicurezza e dimensioni delle chiavi} 

La sicurezza di RSA dipende dall'impossibilità pratica di fattorizzare $n$ quando questo è sufficientemente grande. 
Conoscendo la fattorizzazione $n = pq$, un attaccante potrebbe infatti calcolare $\phi(n)$ e di conseguenza la chiave privata $d$.

Attualmente, le dimensioni delle chiavi considerate sicure sono di almeno 2048 bit, con raccomandazioni crescenti verso 4096 bit per applicazioni che richiedono sicurezza a lungo termine. Chiavi di 1024 bit sono considerate obsolete e vulnerabili ad attacchi con risorse computazionali moderne.

\subsubsection{Crittografia su Curve Ellittiche (ECC)}

La crittografia su curve ellittiche, sviluppata indipendentemente da Neal Koblitz e Victor Miller nel 1985, offre un'alternativa matematicamente elegante e computazionalmente efficiente a RSA.

\paragraph{Fondamenti matematici} Una curva ellittica su un campo finito $\mathbb{Z}_p$ (con $p$ primo e $p > 3$) è definita dall'equazione di Weierstrass in forma normale:
$$y^2 = x^3 + ax + b$$
dove $a, b \in \mathbb{Z}_p$ soddisfano la condizione $4a^3 + 27b^2 \bmod p \neq 0$, che garantisce l'assenza di punti singolari sulla curva.

L'insieme dei punti $(x, y)$ che soddisfano questa equazione, insieme al punto all'infinito $\mathcal{O}$, forma un gruppo abeliano additivo. È possibile definire un'operazione di addizione tra punti della curva tale che, dati due punti $P$ e $Q$, la loro somma $P + Q$ sia ancora un punto della curva.

Per punti distinti $P = (x_P, y_P)$ e $Q = (x_Q, y_Q)$, con $P \neq -Q$, si ha:
$$\lambda = \frac{y_Q - y_P}{x_Q - x_P}, \quad x_S = \lambda^2 - x_P - x_Q, \quad y_S = -y_P + \lambda(x_P - x_S)$$

dove $S = P + Q = (x_S, y_S)$. Nel caso di raddoppio di un punto ($P = Q$), il coefficiente angolare diventa $\lambda = \frac{3x_P^2 + a}{2y_P}$.

\paragraph{Il problema del logaritmo discreto} La sicurezza di ECC si basa sulla difficoltà del problema del logaritmo discreto su curve ellittiche (ECDLP): dati due punti $P$ e $Q$ sulla curva, trovare l'intero $k$ tale che $Q = kP$, dove $kP$ denota l'addizione di $P$ con se stesso $k$ volte.

La moltiplicazione scalare $Q = kP$ è computazionalmente efficiente (tempo polinomiale), mentre il problema inverso è considerato intrattabile: tutti gli algoritmi classici noti hanno complessità esponenziale nella dimensione della chiave.

\paragraph{Vantaggi rispetto a RSA} ECC offre lo stesso livello di sicurezza di RSA con chiavi significativamente più corte. Una chiave ECC di 256 bit fornisce una sicurezza paragonabile a una chiave RSA di 3072 bit. Questo si traduce in:
\begin{itemize}
    \item Minore occupazione di memoria e larghezza di banda
    \item Operazioni crittografiche più veloci
    \item Minore consumo energetico, cruciale per dispositivi mobili e IoT
\end{itemize}

\subsubsection{Sicurezza classica}

Entrambi gli algoritmi sono considerati sicuri nell'ambito del calcolo classico per dimensioni di chiave appropriate. La loro robustezza deriva dalla complessità computazionale dei problemi matematici sottostanti:

\begin{itemize}
    \item \textbf{Fattorizzazione per RSA}: il miglior algoritmo classico noto è il General Number Field Sieve (GNFS), con complessità sub-esponenziale $O(e^{(64/9)^{1/3}(\ln n)^{1/3}(\ln \ln n)^{2/3}})$
    \item \textbf{ECDLP per ECC}: gli algoritmi più efficienti, come il metodo rho di Pollard, hanno complessità completamente esponenziale $O(\sqrt{n})$, dove $n$ è l'ordine del gruppo
\end{itemize}

Questa differenza nella complessità degli attacchi spiega perché ECC richiede chiavi più corte per garantire lo stesso livello di sicurezza.

RSA e ECC costituiscono oggi la base dell'infrastruttura di sicurezza digitale globale, utilizzati in TLS/SSL per la sicurezza web, in SSH per l'accesso remoto sicuro, nella firma digitale di documenti e software, e in numerose altre applicazioni critiche.

\subsection{Limiti rispetto ai computer quantistici}

Avendo introdotto i fondamenti della crittografia classica possiamo provare ora a comprendere 
il funzionamento della crittografia quantistica. Infatti nei prossimi capitoli capiremo quali sono
i vantaggi che i computer quantistici offrono rispetto a quelli classici e come questi
possono compromettere la sicurezza degli algoritmi classici. in particolare esamineremo l'algoritmo 
di Shor e il suo impatto su RSA ed ECC.
\newpage

\section{Computazione quantistica e impatto sulla crittografia classica}

\subsection{Nozioni base di computazione quantistica}

\subsubsection{Richiami essenziali di meccanica quantistica}

La computazione quantistica si fonda su alcuni principi della meccanica quantistica che non trovano un corrispettivo diretto nel modello di calcolo classico.
In questa sezione non si fornisce una trattazione formale della teoria, ma si introducono i concetti essenziali necessari a comprendere il funzionamento dei computer quantistici e il loro impatto sulla crittografia.

Il primo concetto fondamentale è quello di \emph{stato quantistico}. Mentre un bit classico può assumere esclusivamente i valori 0 o 1, un sistema quantistico può trovarsi in una \emph{sovrapposizione} di stati. In particolare, lo stato di un qubit può essere descritto come una combinazione lineare degli stati base $\ket{0}$ e $\ket{1}$, con coefficienti complessi che ne determinano le probabilità di osservazione.

Un secondo concetto chiave è l'\emph{entanglement}, una forma di correlazione quantistica tra più qubit per cui lo stato complessivo del sistema non può essere descritto come il prodotto degli stati dei singoli qubit. In presenza di entanglement, la misura di un qubit influisce istantaneamente sullo stato degli altri, indipendentemente dalla distanza che li separa. Questa proprietà consente di rappresentare e manipolare informazioni in modo non separabile.

Il terzo elemento fondamentale è la \emph{misura}. Quando uno stato quantistico viene misurato, la sovrapposizione collassa e il risultato ottenuto è un valore classico. L'esito della misura è probabilistico e dipende dallo stato del sistema immediatamente prima dell'osservazione.

Sovrapposizione, entanglement e misura costituiscono i principi alla base della differenza concettuale tra computazione classica e computazione quantistica, e determinano il modo in cui l'informazione viene elaborata in un sistema quantistico.

\subsubsection{Qubit e operazioni quantistiche}

L'unità fondamentale di informazione in un computer quantistico è il qubit.
Dal punto di vista matematico, un qubit è rappresentato come un vettore unitario in uno spazio di Hilbert bidimensionale, mentre un registro composto da $n$ qubit è descritto in uno spazio di dimensione $2^n$.

Le operazioni sui qubit sono realizzate mediante \emph{porte quantistiche}, che corrispondono a trasformazioni lineari e reversibili, descritte da operatori unitari. A differenza delle porte logiche classiche, le porte quantistiche agiscono su stati in sovrapposizione, modificando simultaneamente tutte le componenti dello stato quantistico.

Questo comportamento consente di elaborare, in un singolo passo computazionale, un insieme di stati che cresce esponenzialmente con il numero di qubit. Tale fenomeno è noto come \emph{parallelismo quantistico} e rappresenta una delle principali fonti di vantaggio rispetto al calcolo classico, pur non traducendosi automaticamente in un'accelerazione per qualsiasi problema.

\subsubsection{Modello di calcolo quantistico}

Il funzionamento di un computer quantistico segue un modello di calcolo specifico, distinto da quello deterministico utilizzato nei computer classici.
Un algoritmo quantistico è generalmente articolato in tre fasi principali:
\begin{itemize}
    \item preparazione dello stato iniziale;
    \item applicazione di una sequenza di porte quantistiche;
    \item misura finale del sistema.
\end{itemize}

Durante la fase di evoluzione, lo stato quantistico del sistema viene trasformato in modo deterministico secondo le leggi della meccanica quantistica. La natura probabilistica emerge esclusivamente al momento della misura, quando lo stato viene proiettato su uno dei possibili risultati osservabili.

La potenza del modello di calcolo quantistico non risiede nella possibilità di valutare esplicitamente tutte le soluzioni di un problema, bensì nella capacità di sfruttare il fenomeno dell'\emph{interferenza quantistica}. Attraverso opportune sequenze di operazioni, le ampiezze associate alle soluzioni corrette possono essere amplificate, mentre quelle delle soluzioni errate vengono attenuate.

Questo meccanismo è alla base del funzionamento di algoritmi quantistici noti, come l'algoritmo di Shor, che riescono a risolvere efficientemente problemi matematici considerati intrattabili nel modello di calcolo classico.

\subsubsection{Collegamento con la crittografia}

I concetti introdotti in questa sezione costituiscono il fondamento teorico per analizzare l'impatto della computazione quantistica sulla crittografia moderna.
In particolare, l'uso combinato di sovrapposizione, entanglement e interferenza consente di affrontare in modo efficiente problemi come la fattorizzazione di interi e il calcolo del logaritmo discreto.

Poiché tali problemi sono alla base della sicurezza di numerosi sistemi crittografici asimmetrici, la computazione quantistica rappresenta una minaccia diretta per protocolli ampiamente utilizzati. Nel prossimo paragrafo verrà analizzato l'algoritmo di Shor, mostrando in che modo il modello di calcolo quantistico renda concretamente possibile la loro compromissione.

\subsubsection{Computer quantistici reali e sfide di realizzazione}

Un computer quantistico reale è un sistema fisico composto da qubit che devono mantenere le loro proprietà quantistiche per tutta la durata del calcolo. Le tecnologie attualmente più diffuse si basano su qubit superconduttori, che operano a temperature estremamente basse, prossime allo zero assoluto, tipicamente dell'ordine di alcune decine di millikelvin, al fine di ridurre il rumore termico e preservare la coerenza quantistica.

Il raggiungimento di tali condizioni richiede l'utilizzo di refrigeratori a diluizione, dispositivi complessi e costosi che rappresentano un primo limite alla scalabilità dei sistemi. Anche minime interferenze ambientali possono causare la \emph{decoerenza}, ovvero la perdita delle proprietà quantistiche dei qubit, compromettendo la correttezza del calcolo.

Un'ulteriore difficoltà riguarda il controllo delle operazioni quantistiche. Le porte devono essere applicate con estrema precisione, poiché errori anche molto piccoli tendono ad accumularsi rapidamente con l'aumentare del numero di qubit e della profondità del circuito.

Per mitigare questo problema si ricorre a tecniche di \emph{correzione d'errore quantistica}. Tuttavia, gli schemi attualmente noti richiedono l'impiego di un numero elevato di qubit fisici per realizzare un singolo qubit logico affidabile, tipicamente dell'ordine di centinaia o migliaia.

A causa dei limiti tecnologici attuali, in particolare dei tempi di coerenza ridotti e dell'elevato tasso di errore, l'esecuzione di algoritmi quantistici complessi su larga scala, come quelli necessari per rompere RSA a 2048 bit, non è ancora praticabile.

Per queste ragioni si ritiene che saranno necessari ancora diversi anni, se non decenni, prima che computer quantistici in grado di compromettere concretamente i sistemi crittografici attuali diventino disponibili. Nonostante ciò, i progressi tecnologici e gli investimenti in corso rendono questa prospettiva rilevante dal punto di vista della sicurezza a lungo termine.

\subsection{Algoritmi quantistici rilevanti: Shor}

L'algoritmo di Shor rappresenta uno dei risultati più significativi nel campo della computazione quantistica, non solo per le sue implicazioni sulla crittografia, ma anche per la profondità delle idee matematiche che lo compongono. Per comprendere appieno il funzionamento di questo algoritmo, è necessario procedere con un'analisi rigorosa che parta dai fondamenti matematici, prescindendo inizialmente dagli aspetti quantistici. L'obiettivo di questa sezione è chiarire perché l'algoritmo funziona dal punto di vista matematico, riservando la discussione su come viene accelerato dal computer quantistico a una fase successiva dell'analisi.

\subsubsection{Il problema della fattorizzazione}

Il punto di partenza è il problema della fattorizzazione di interi. Dato un intero composto $N = p \cdot q$, dove $p$ e $q$ sono numeri primi di grandi dimensioni, il problema consiste nel determinare $p$ e $q$ conoscendo solamente $N$. Questo problema costituisce il fondamento della sicurezza di numerosi sistemi crittografici classici, in particolare del sistema RSA. La difficoltà computazionale della fattorizzazione deriva da due caratteristiche fondamentali: in primo luogo, non è noto alcun algoritmo classico in grado di fattorizzare un numero in tempo polinomiale rispetto alla dimensione dell'input; in secondo luogo, i tentativi diretti di fattorizzazione crescono rapidamente con l'aumentare della dimensione di $N$, rendendo il problema intrattabile per valori sufficientemente grandi.

L'intuizione fondamentale di Shor consiste nel trasformare il problema della fattorizzazione in un problema di natura diversa, caratterizzato da una struttura matematica più favorevole. Questa trasformazione permette di ricondurre la ricerca dei fattori primi a un problema di teoria dei numeri che, come vedremo, può essere risolto in modo efficiente sfruttando le peculiarità della computazione quantistica.

\subsubsection{Dalla fattorizzazione alla teoria dei numeri}

La strategia di Shor si basa sull'osservazione che la fattorizzazione può essere ricondotta al problema della determinazione del periodo di una funzione. Questa connessione non è immediata e richiede una serie di passaggi matematici che è necessario esplicitare con precisione.

Il primo passo consiste nella scelta di un numero intero $a$ tale che $1 < a < N$ e $\gcd(a, N) = 1$. La condizione sulla coprimalità è essenziale: se infatti $\gcd(a, N) \neq 1$, avremmo già trovato un fattore non banale di $N$ semplicemente calcolando il massimo comun divisore. Una volta scelto $a$, si considera la funzione esponenziale modulare definita come:
\[
f(x) = a^x \bmod N
\]

Questa funzione possiede una proprietà fondamentale: è periodica. Più precisamente, esiste un minimo intero $r > 0$ tale che:
\[
a^r \equiv 1 \pmod{N}
\]

Questo valore $r$ è chiamato ordine di $a$ modulo $N$. L'esistenza di tale periodo è garantita dal teorema di Eulero, secondo cui $a^{\phi(N)} \equiv 1 \pmod{N}$, dove $\phi(N)$ è la funzione di Eulero. Il periodo cercato è dunque un divisore di $\phi(N)$.

\subsubsection{L'utilità del periodo per la fattorizzazione}

A questo punto sorge naturale una domanda: perché la conoscenza del periodo $r$ dovrebbe aiutarci a fattorizzare $N$? La risposta risiede in una proprietà algebrica profonda che collega l'ordine di un elemento alla struttura moltiplicativa del gruppo $(\mathbb{Z}/N\mathbb{Z})^*$.

Se il periodo $r$ soddisfa due condizioni specifiche, ovvero se $r$ è pari e se $a^{r/2} \not\equiv -1 \pmod{N}$, allora è possibile estrarre fattori non banali di $N$ calcolando:
\[
\gcd(a^{r/2} - 1, N) \quad \text{e} \quad \gcd(a^{r/2} + 1, N)
\]

Questa è la chiave matematica dell'intero algoritmo. Per comprendere perché questa procedura funziona, è necessario esaminare più da vicino la struttura algebrica sottostante. Dalla relazione $a^r \equiv 1 \pmod{N}$ segue immediatamente che:
\[
a^r - 1 \equiv 0 \pmod{N}
\]

Poiché $r$ è pari per ipotesi, possiamo fattorizzare il termine $a^r - 1$ utilizzando la differenza di quadrati:
\[
a^r - 1 = (a^{r/2})^2 - 1 = (a^{r/2} - 1)(a^{r/2} + 1)
\]

Ne consegue che:
\[
(a^{r/2} - 1)(a^{r/2} + 1) \equiv 0 \pmod{N}
\]

Questa congruenza ci dice che $N$ divide il prodotto $(a^{r/2} - 1)(a^{r/2} + 1)$. Tuttavia, per ipotesi, $a^{r/2} \not\equiv -1 \pmod{N}$, il che significa che $a^{r/2} + 1$ non è divisibile per $N$. Analogamente, poiché $r$ è il periodo minimo, anche $a^{r/2} - 1$ non può essere divisibile per $N$ (altrimenti $r/2$ sarebbe il periodo). Di conseguenza, $N$ divide il prodotto di due fattori senza dividere ciascun fattore singolarmente. Questo implica necessariamente che ciascuno dei due fattori condivide almeno un divisore primo con $N$, ma non tutti. Il calcolo del massimo comun divisore permette quindi di estrarre questi divisori non banali.

\subsubsection{Esempio numerico completo}

Per rendere concreti i concetti esposti, consideriamo un esempio numerico completo. Supponiamo di voler fattorizzare $N = 15$. Scegliamo $a = 2$ e verifichiamo che $\gcd(2, 15) = 1$, come richiesto. Procediamo quindi al calcolo della sequenza di potenze di $a$ modulo $N$:

\begin{center}
\begin{tabular}{c|c}
$x$ & $2^x \bmod 15$ \\
\hline
1 & 2 \\
2 & 4 \\
3 & 8 \\
4 & 1
\end{tabular}
\end{center}

Osserviamo che il valore si ripete dopo quattro iterazioni, pertanto il periodo è $r = 4$. Verifichiamo ora le condizioni necessarie: il periodo è effettivamente pari, e calcoliamo $a^{r/2} = 2^2 = 4$. Poiché $4 \not\equiv -1 \pmod{15}$ (infatti $4 \equiv 4 \pmod{15}$), entrambe le condizioni sono soddisfatte. Possiamo quindi applicare la formula per estrarre i fattori:
\begin{align*}
\gcd(4 - 1, 15) &= \gcd(3, 15) = 3 \\
\gcd(4 + 1, 15) &= \gcd(5, 15) = 5
\end{align*}

Abbiamo così ottenuto la fattorizzazione $15 = 3 \cdot 5$.

\subsubsection{La complessità computazionale del metodo}

A questo punto è importante sottolineare quali passaggi del metodo siano effettivamente computazionalmente trattabili e quale rappresenti invece il collo di bottiglia. I passaggi che non presentano difficoltà computazionali significative includono il calcolo del massimo comun divisore, che può essere eseguito efficientemente tramite l'algoritmo di Euclide, e l'esecuzione di esponenziazioni modulari, che può essere realizzata in tempo polinomiale mediante l'algoritmo di esponenziazione veloce.

Il passaggio critico dell'intero procedimento è la determinazione del periodo $r$. Nel contesto della computazione classica, il calcolo del periodo della funzione $a^x \bmod N$ richiede essenzialmente di valutare la funzione ripetutamente fino a quando non si osserva la ripetizione del valore iniziale. Il numero di valutazioni necessarie nel caso peggiore può essere dell'ordine di $N$, e anche nel caso medio il numero di operazioni cresce in modo tale da rendere il problema intrattabile per valori di $N$ sufficientemente grandi. È precisamente in questo passaggio che risiede il collo di bottiglia computazionale che impedisce alla fattorizzazione classica di essere efficiente.

\subsubsection{Casi di fallimento e strategie di gestione}

È importante osservare che il metodo descritto non ha sempre successo. Possono verificarsi due situazioni in cui la procedura non produce una fattorizzazione: il periodo $r$ potrebbe risultare dispari, oppure potrebbe accadere che $a^{r/2} \equiv -1 \pmod{N}$. In entrambi questi casi, non è possibile applicare la fattorizzazione attraverso il massimo comun divisore nel modo descritto. Tuttavia, è possibile dimostrare che la probabilità di incorrere in uno di questi casi sfavorevoli è relativamente bassa. Inoltre, qualora si verifichi uno di questi casi, è sufficiente scegliere un valore diverso di $a$ e ripetere l'intero procedimento. Con alta probabilità, dopo pochi tentativi si otterrà un periodo che soddisfa le condizioni necessarie.

\subsubsection{Sintesi del contributo matematico}

Ricapitolando quanto esposto, dal punto di vista strettamente matematico, l'algoritmo di Shor opera attraverso una sequenza logica di trasformazioni: in primo luogo, trasforma il problema della fattorizzazione in un problema di determinazione della periodicità di una funzione; in secondo luogo, sfrutta proprietà fondamentali dell'aritmetica modulare per collegare il periodo alla struttura dei fattori di $N$; in terzo luogo, riduce il problema al calcolo dell'ordine di un elemento modulo $N$; infine, estrae i fattori cercati attraverso il calcolo del massimo comun divisore.

È fondamentale sottolineare che fino a questo punto non è stato introdotto alcun concetto di natura quantistica. Tutto quanto discusso appartiene al dominio della matematica classica e della teoria dei numeri. Il contributo essenziale della computazione quantistica risiede nella capacità di rendere efficiente il passaggio centrale, ovvero la determinazione del periodo. Mentre su un computer classico questo passaggio è computazionalmente intrattabile per numeri di grandi dimensioni, vedremo che un computer quantistico è in grado di eseguirlo in tempo polinomiale, rendendo così l'intero algoritmo efficiente e ponendo una seria minaccia alla sicurezza dei sistemi crittografici basati sulla difficoltà della fattorizzazione.

\subsubsection{La trasformata di Fourier quantistica}

Per comprendere come un computer quantistico risolva efficientemente il problema della determinazione del periodo, è necessario introdurre uno strumento matematico fondamentale: la trasformata di Fourier. L'obiettivo di questa parte dell'analisi è chiarire perché la trasformata di Fourier rappresenti lo strumento naturale per affrontare problemi di periodicità e come la sua versione quantistica costituisca l'elemento centrale dell'algoritmo di Shor. Procederemo costruendo il ragionamento a partire da concetti elementari, senza assumere familiarità con gli aspetti tecnici della trasformata.

\paragraph{Il ruolo della trasformata di Fourier nell'analisi della periodicità}

Dall'analisi matematica precedente è emerso che il problema centrale consiste nel determinare il periodo di una funzione della forma $f(x) = a^x \bmod N$. È importante chiarire che il calcolo dei singoli valori di $f(x)$ non presenta difficoltà computazionali: dato un valore specifico di $x$, è possibile calcolare $f(x)$ in modo efficiente. La difficoltà risiede nella determinazione della frequenza con cui i valori della funzione si ripetono.

La trasformata di Fourier è uno strumento matematico che permette di operare un cambio di prospettiva fondamentale. Concettualmente, essa consente di passare da una descrizione di un segnale o di una funzione nel cosiddetto ``dominio del tempo'' a una descrizione nel ``dominio delle frequenze''. Il periodo di una funzione è intrinsecamente una proprietà legata alle frequenze: una funzione periodica con periodo $r$ può essere interpretata come un segnale che oscilla con frequenza fondamentale $1/r$. Questa osservazione costituisce la chiave per comprendere perché la trasformata di Fourier sia lo strumento appropriato per estrarre informazioni sulla periodicità.

\paragraph{Intuizione alla base della trasformata di Fourier}

Prima di procedere con la formulazione matematica precisa, è utile sviluppare un'intuizione del funzionamento della trasformata di Fourier attraverso un esempio semplice. Si consideri un segnale periodico, che potrebbe rappresentare un'onda sonora, una vibrazione meccanica o qualsiasi altra grandezza fisica che varia nel tempo secondo un pattern ripetitivo. Nel dominio temporale, osserviamo come il segnale evolve istante per istante, registrando il valore della grandezza in funzione del tempo.

La trasformata di Fourier effettua un'operazione concettualmente diversa: prende il segnale nel dominio temporale e lo scompone in una somma di oscillazioni elementari, ciascuna caratterizzata da una frequenza ben definita. In altre parole, invece di descrivere quando il segnale assume determinati valori, la trasformata descrive quali frequenze compongono il segnale e con quale intensità ciascuna frequenza contribuisce. Il periodo del segnale originale emerge naturalmente da questa rappresentazione in termini di frequenze.

Per rendere questa idea più concreta, si consideri un segnale definito come la somma di due sinusoidi:
\[
s(t) = \sin(2\pi t) + \sin(4\pi t)
\]

Nel dominio temporale, questo segnale appare come un'onda dalla forma piuttosto irregolare, risultante dalla sovrapposizione delle due componenti. Tuttavia, nel dominio delle frequenze, la trasformata di Fourier rivela immediatamente la struttura sottostante: il segnale è composto da esattamente due componenti, una con frequenza $1$ e una con frequenza $2$. La trasformata di Fourier serve precisamente a estrarre questa decomposizione in componenti frequenziali, permettendo di identificare le frequenze fondamentali che caratterizzano il segnale.

\paragraph{La trasformata di Fourier discreta}

Nel contesto dell'algoritmo di Shor, non abbiamo a che fare con segnali continui nel tempo, ma piuttosto con sequenze discrete di valori. La funzione $f(x) = a^x \bmod N$ produce una sequenza di valori interi $f(0), f(1), f(2), \ldots$ che si ripete con periodo $r$. Per analizzare questo tipo di segnali discreti è necessario utilizzare la trasformata di Fourier discreta, comunemente indicata con l'acronimo DFT (Discrete Fourier Transform).

La trasformata di Fourier discreta opera su una sequenza finita di valori $x_0, x_1, \ldots, x_{N-1}$ e la trasforma in un'altra sequenza $X_0, X_1, \ldots, X_{N-1}$, dove ciascun coefficiente $X_k$ misura l'intensità con cui la frequenza $k$ è presente nel segnale originale. La definizione formale della trasformata di Fourier discreta è data da:
\[
X_k = \sum_{n=0}^{N-1} x_n \, e^{-2\pi i kn/N}
\]

Sebbene non sia necessario memorizzare questa formula nei dettagli, è importante comprenderne il significato concettuale. La sommatoria confronta il segnale originale con tutte le possibili oscillazioni discrete di frequenza $k$, e il valore di $X_k$ misura quanto bene l'oscillazione di frequenza $k$ si ``accorda'' con il segnale. Le oscillazioni che sono in fase con il segnale contribuiscono costruttivamente, mentre quelle fuori fase tendono a cancellarsi.

\paragraph{Periodicità e concentrazione spettrale}

Il collegamento diretto tra periodicità e trasformata di Fourier emerge quando si considera il caso di una funzione perfettamente periodica. Se una funzione possiede periodo $r$, la sua trasformata di Fourier non è distribuita uniformemente su tutte le frequenze, ma risulta concentrata su valori specifici. In particolare, l'energia spettrale è localizzata in corrispondenza di frequenze che sono multipli interi di $1/r$.

Questa proprietà implica che, osservando il risultato della trasformata di Fourier, è possibile ricavare informazioni precise sul periodo $r$. I picchi nella trasformata identificano le frequenze caratteristiche, e da queste frequenze si può risalire al periodo. Tuttavia, nel contesto della computazione classica, il calcolo della trasformata di Fourier discreta richiede un numero di operazioni che, anche utilizzando algoritmi efficienti come la FFT (Fast Fourier Transform), cresce almeno proporzionalmente al numero di campioni. Per sequenze di lunghezza comparabile a $N$, questo rappresenta comunque un costo computazionale significativo che non risolve il problema della complessità della fattorizzazione.

\paragraph{Dalla DFT alla trasformata di Fourier quantistica}

La trasformata di Fourier quantistica, comunemente indicata con l'acronimo QFT (Quantum Fourier Transform), rappresenta l'adattamento della trasformata di Fourier discreta al contesto della computazione quantistica. È fondamentale comprendere che la QFT non introduce nuove idee matematiche: si tratta essenzialmente della stessa trasformazione matematica definita dalla DFT, ma applicata a stati quantistici invece che a sequenze di numeri classici.

In un computer quantistico, una sequenza di valori viene rappresentata attraverso una sovrapposizione quantistica, ovvero uno stato della forma:
\[
\sum_x \alpha_x \ket{x}
\]

dove i coefficienti complessi $\alpha_x$ rappresentano le ampiezze di probabilità associate a ciascun valore di base $\ket{x}$. La QFT trasforma questo stato in un nuovo stato:
\[
\sum_k \beta_k \ket{k}
\]

dove i coefficienti $\beta_k$ sono esattamente la trasformata di Fourier dei coefficienti originali $\alpha_x$. In altre parole, la QFT realizza fisicamente, a livello di stato quantistico, la stessa operazione matematica che la DFT realizza a livello di array di numeri.

\paragraph{Il vantaggio computazionale della QFT}

La ragione per cui la QFT rappresenta un avanzamento rivoluzionario rispetto alla DFT classica risiede in una proprietà fondamentale della computazione quantistica: la capacità di operare simultaneamente su una sovrapposizione di stati. Nel computer quantistico, tutti i coefficienti $\alpha_x$ esistono contemporaneamente nella sovrapposizione, e la QFT agisce sull'intera sovrapposizione in un'unica operazione coerente.

Il risultato di questa applicazione coerente della trasformata è che le ampiezze dei diversi stati interferiscono tra loro secondo le leggi della meccanica quantistica. Le frequenze che sono compatibili con la periodicità della funzione originale vengono amplificate attraverso interferenza costruttiva, mentre le frequenze incompatibili vengono soppresse attraverso interferenza distruttiva. Quando si effettua una misura sullo stato risultante, si ottiene con alta probabilità un valore che contiene informazione sul periodo cercato.

È importante sottolineare che la misura non fornisce direttamente il periodo $r$, ma piuttosto un valore dal quale è possibile estrarre $r$ utilizzando tecniche matematiche classiche. In particolare, si utilizza l'algoritmo delle frazioni continue per approssimare il rapporto misurato con una frazione che ha il periodo come denominatore. Questa fase di post-elaborazione classica è essenziale per completare l'algoritmo.

\paragraph{Esempio concettuale del meccanismo}

Per rendere più concreto il meccanismo di estrazione del periodo, consideriamo un esempio semplificato. Supponiamo che la funzione analizzata abbia periodo $r = 4$ e che il registro quantistico abbia dimensione $Q$. Dopo l'applicazione della QFT, lo stato quantistico non è distribuito uniformemente, ma presenta concentrazioni di ampiezza in corrispondenza di valori approssimabili come:
\[
k \approx \frac{m}{4} \cdot Q
\]

dove $m$ è un intero. Quando si effettua la misura, si ottiene con alta probabilità uno di questi valori di $k$. Dal rapporto:
\[
\frac{k}{Q} \approx \frac{m}{r}
\]

è possibile ricostruire il periodo $r$ attraverso l'approssimazione con frazioni continue. Questo esempio illustra il collegamento diretto tra l'applicazione della QFT, l'estrazione di informazione sulla periodicità attraverso la misura quantistica, e il recupero finale del periodo che permette la fattorizzazione.

\paragraph{La natura del vantaggio quantistico}

È essenziale chiarire un aspetto concettuale che spesso genera confusione. Il computer quantistico non ottiene il suo vantaggio semplicemente ``provando tutte le frequenze in parallelo'' né ``calcolando la trasformata esplicitamente come farebbe una CPU classica''. Il meccanismo è qualitativamente diverso: il computer quantistico prepara uno stato quantistico con una struttura specifica, sfrutta le proprietà di interferenza e sovrapposizione per far emergere le frequenze rilevanti, e produce il periodo come risultato di un processo probabilistico governato dalle leggi della meccanica quantistica.

La QFT è il meccanismo matematico che rende possibile questa interferenza controllata. Essa permette di organizzare le ampiezze quantistiche in modo tale che, al momento della misura, i valori che contengono informazione sul periodo abbiano probabilità di osservazione significativamente maggiore rispetto agli altri. Questa capacità di manipolare coerentemente le ampiezze di probabilità attraverso interfer

\section{Crittografia post quantistica}

\subsection{Definizione, requisiti e obiettivi}
\paragraph{Requisiti pratici e tempistiche}

Sebbene la minaccia sia teoricamente dimostrata, la realizzazione pratica di computer quantistici capaci di violare RSA ed ECC richiede risorse considerevoli. Secondo stime del NIST, per compromettere una chiave RSA-2048 sarebbero necessari diversi milioni di qubit logici affidabili, mentre le implementazioni attuali (2024) operano con centinaia di qubit fisici caratterizzati da elevati tassi di errore.

La transizione da qubit fisici a qubit logici richiede tecniche di correzione degli errori quantistici che impongono un overhead significativo: potrebbero essere necessari da centinaia a migliaia di qubit fisici per realizzare un singolo qubit logico stabile.

Nonostante queste difficoltà tecnologiche, il principio "harvest now, decrypt later" rappresenta una preoccupazione concreta: un attaccante potrebbe intercettare e archiviare oggi comunicazioni cifrate con RSA o ECC, per decifrarle in futuro quando disporrà di computer quantistici sufficientemente potenti. Questa considerazione è particolarmente rilevante per dati che richiedono riservatezza a lungo termine, come informazioni mediche, segreti industriali o comunicazioni governative.

\subsection{Ruolo del Nist e processo di standardizzazione}
\paragraph{La risposta: crittografia post-quantistica}

Di fronte a questa minaccia emergente, il National Institute of Standards and Technology (NIST) ha avviato nel 2016 un processo di standardizzazione per identificare algoritmi crittografici resistenti agli attacchi quantistici. Nel luglio 2022, il NIST ha annunciato i primi algoritmi selezionati per la standardizzazione, basati su problemi matematici ritenuti difficili anche per computer quantistici, come i reticoli algebrici e i codici correttori di errori.

La migrazione verso la crittografia post-quantistica rappresenta una delle sfide più urgenti per la sicurezza informatica moderna, richiedendo un'attenta pianificazione per sostituire l'infrastruttura crittografica esistente mantenendo retrocompatibilità e garantendo una transizione graduale e sicura.

\newpage

\section{Panorama sugli algoritmi post-quantistici}

\bibliographystyle{plain}
\bibliography{bibliography}

\end{document}